参数估计有两种形式：点估计与区间估计。本文选择几种常用的点估计方法作一些讨论。

用于估计未知参数的统计量称为**点估计（量）**。参数 $\theta$ 的估计量常用 $\hat{\theta} = \hat{\theta}(x_{1},x_{2}, \dots, x_{n})$ 表示，参数 $\theta$ 的可能取值范围称为**参数空间**，记为 $\Theta = \{\theta\}$。

@[toc]
# 最大似然估计
最大似然估计，即对**似然函数**最大化，其关键是从样本 $x$ 和含有位置参数 $\theta$ 的分布 $p(x,\theta)$ 获得似然函数。设 $x=(x_{1},x_{2},\dots,x_{n})$ 是来自含有未知参数的某分布 $p(x,\theta)$ 的一个样本，那么其联合分布为：
$$ p(x,\theta) = \prod_{i=1}^{n}p(x_{i},\theta) $$ 其中 $p(x_{i},\theta)$ 在连续场合是指密度函数在 $x_{i}$ 处的值，在离散场合为分布列中的一个概率 $P_{\theta}(X=x_{i})$ 。对样本分布 $p(x,\theta)$ 我们知道：

 1. 样本如何产生？先有 $\theta$ 后有 $x$，即先有一个给定的 $\theta$ 的值 $\theta_{0}$，然后由分布 $p(x,\theta_{0})$ 经过随机抽样产生样本观察值 $x$。
 2. 如今我们有了 $x$ 如何追溯参数 $\theta_{0}$ 呢？当给定样本观察值 $x$ 时样本分布 $p(x,\theta)$ 仅是 $\theta$ 的函数，可记为 $L(\theta,x)$ 或 $L(\theta)$，并称其为**似然函数**。对于不同的 $\theta_{1},\theta_{2}\in\Theta$，可使得样本观察值 $x$ 出现的机会不同。若 $L(\theta_{1}) > L(\theta_{2})$，表明 $\theta_{1}$ 会使 $x$ 出现的机会比 $\theta_{2}$ 更大些，即 $\theta_{1}$ 比 $\theta_{2}$ 更像真值 $\theta_{0}$。也就是说 $L(\theta)$ 成为了度量 $\theta$ 更像真值的程度，其值越大越像。按此思路，在参数空间 $\Theta$ 中使 $L(\theta)$ 最大的 $\hat{\theta}$ 就是最像 $\theta_{0}$ 的真值，这个 $\hat{\theta}$ 就是 $\theta$ 的**最大似然估计**。

这里给出两个实例。

1.**伯努利分布**实例

假设 $P(X=1)=p,P(X=0)=1-p$ 综合起来就有
$$P(X)=p^{X}(1-p)^{1-X}$$
此时如果有一组数据 $D$ 是从这个随机变量中采样得到的，那么就有
$$
\begin{aligned}
\ max_{p}\log P(D)&= \max_{p}\log\prod_{i}^{N}P(D_{i}) \\
        &=\max_{p}\sum_{i}^{N}\log P(D_{i}) \\
        &=\max_{p}\sum_{i}^{N}[D_{i}\log p+(1-D_{i})\log(1-p)]
\end{aligned}
$$
对上式求导，则有
$$
\nabla_{p}\max_{p}\log P(D)=\sum_{i}^{N}[D_{i}\frac{1}{p}+(1-D_{i})\frac{1}{p-1}]
$$
求极值，令导数为 $0$，就有
$$
\begin{aligned}
& \sum_{i}^{N}[D_{i}\frac{1}{p}+(1-D_{i})\frac{1}{p-1}]=0 \\
& \sum_{i}^{N}[D_{i}(p-1)+(1-D_{i})p]=0 \\
& \sum_{i}^{N}(p-D_{i})=0 \\
& p=\frac{1}{N}\sum_{i}^{N}D_{i}
\end{aligned}
$$
即全部采样的平均值。

2.**高斯分布**实例

令 $p(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}$，采用同样的方法有
$$
\begin{aligned}
 \max_{p}\log P(D) &= \max_{p}\log\prod_{i}^{N}P(D_{i})  \\
		 &= \max_{p}\sum_{i}^{N}\log P(D_{i}) \\
		 &= \max_{p}\sum_{i}^{N}[-\frac{1}{2}\log(2\pi\sigma^{2})-\frac{(D_{i}-\mu)^{2}}{2\sigma^{2}}] \\
		 &= \max[-\frac{N}{2}\log(2\pi\sigma^{2})-\frac{1}{2\sigma^{2}}\sum_{i}^{N}(D_{i}-\mu)^{2}]
 \end{aligned}
$$
此处包含两个参数，分别估计。

首先对 $\mu$ 求导，有
$$
\frac{\partial\max_{\mu}\log P(D)}{\partial \mu} = -\frac{1}{\sigma^{2}}\sum_{i}^{N}(\mu-D_{i})
$$
令导数为 $0$，有
$$
-\frac{1}{\sigma^{2}}\sum_{i}^{N}(\mu-D_{i})=0,\quad \mu=\frac{1}{N}\sum_{i}^{N}D_{i}
$$
注意很容易看出这个结果与最小二乘计算完全相同，实质上最小二乘法可以与极大似然法中假定误差遵循正态分布的特殊情况相对应。

其次对 $\sigma^{2}$ 求导，有
$$
\frac{\partial\max_{\sigma^{2}}\log P(D)}{\partial\sigma^{2}} = -\frac{N}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{i}^{N}(D_{i}-\mu)^{2}
$$
令导数为 $0$，有
$$
-\frac{N}{2\sigma^{2}}+\frac{1}{4\sigma^{4}}\sum_{i}^{N}(D_{i}-\mu)^{2}=0
$$
$$
\sigma^{2} = \frac{1}{N}\sum_{i}^{N}(D_{i}-\mu)^{2}
$$
可见最终计算结果与期望方差计算方式完全一致。注意最大似然估计并不一定具有无偏性。

对似然函数添加或剔除一个与参数 $\theta$ 无关的量 $c(x)>0$，不影响寻求最大似然估计的最终结果，故 $c(x)L(\theta)$ 仍然是 $\theta$ 的似然函数。例如，对于正态分布而言：
$$ L(\mu,\sigma^{2}) = \prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma^{2}}e^{-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}} \propto (\sigma^{2})^{-\frac{n}{2}}\exp\left\{-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right\} $$

**不变原理**： 设 $X\sim p(x,\theta), \theta\in\Theta$，若 $\theta$ 的最大似然估计为 $\hat{\theta}$ ，则对任意函数 $\gamma=g(\theta)$，$\gamma$ 的最大似然估计为 $\hat{\gamma}=g(\hat{\theta})$。 

# 贝叶斯估计

统计学中有两个主要学派：频率学派（又称经典学派）和贝叶斯学派。前述最大似然估计属于经典统计学范畴。频率学派利用**总体信息**和**样本信息**进行统计推断，贝叶斯学派与之的区别在于还用到了**先验信息**。

贝叶斯学派最基本的观点是：任一未知量 $\theta$ 都可以看做随机变量，可用一个概率分布区描述，这个分布称为**先验分布** （记为 $\pi(\theta)$）。因为任一未知量都有不确定性，而在表述不确定性地程度时，概率与概率分布是最好的语言。依赖于参数 $\theta$ 的密度函数在经典统计学中记为 $p(x,\theta)$，它表示参数空间 $\Theta$ 中不同的 $\theta$ 对应不同的分布。在贝叶斯统计中应记为 $p(x|\theta)$ ，表示随机变量 $\theta$ 给定某个值时，$X$ 的条件密度函数。

从贝叶斯观点看，样本 $x$ 的产生要分两步进行：首先，设想从先验分布 $\pi(\theta)$ 中产生一个样本 $\theta'$ ，这一步人是看不到的，所以是“设想”；再从 $p(x|\theta')$ 中产生一个样本 $x=(x_{1},x_{2},x_{3},\dots,x_{n})$ 。这时样本 $x$ 的联合条件密度函数为：
$$ p(x|\theta')=\prod_{i=1}^{n}p(x_{i}|\theta') $$ 这个联合分布综合了**总体信息**和**样本信息**，又称为**似然函数**。它与极大似然估计中的似然函数没有什么区别。$\theta'$ 仍然是未知的，它是按照先验分布 $\pi(\theta)$ 产生的，为了把**先验信息**综合进去，不能只考虑 $\theta'$，对 $\theta$ 的其它值发生的可能性也要加以考虑，故要用 $\pi(\theta)$ 进行综合。这样一来，样本 $x$ 和参数 $\theta$ 的联合分布为：
$$ h(x,\theta)=p(x|\theta)\pi(\theta) $$ 这个联合分布综合了**总体信息**、**样本信息**和**先验信息**。

我们的核心目标是对 $\theta$ 进行估计，若把 $h(x,\theta)$ 作如下分解：
$$ h(x,\theta) = \pi(\theta|x)m(x) $$ 其中 $m(x)$ 是 $X$ 的**边际密度函数**:
$$ m(x) = \int_{\Theta}h(x,\theta)\mathrm{d}\theta = \int_{\Theta}p(x|\theta)\pi(\theta)\mathrm{d}\theta $$ 它与 $\theta$ 无关。因此，能用来对 $\theta$ 进行估计的只有条件分布 $\pi(\theta|x)$，它的计算公式是：
$$ \pi(\theta|x)=\frac{h(x,\theta)}{m(x)} = \frac{p(x|\theta)\pi(\theta)}{m(x)} =  \frac{p(x|\theta)\pi(\theta)}{\int_{\Theta}p(x|\theta)\pi(\theta)\mathrm{d}\theta} $$ 这就是**贝叶斯公式的密度函数形式**。 这个条件分布称为 $\theta$ 的**后验分布**，它集中了**总体信息**、**样本信息**和**先验信息**中有关 $\theta$ 的一切信息。也可以说是总体和样本对先验分布 $\pi(\theta)$ 作调整的结果，比先验分布更接近 $\theta$ 的实际情况。上述公式是在 $x$ 和 $\theta$ 都是连续随机变量场合下的贝叶斯公式。其它场合下的贝叶斯公式如下：

 1. $x$ 离散，$\theta$ 连续： $$\pi(\theta|x_{j})=\frac{p(x_{j}|\theta)\pi(\theta)}{\int_{\Theta}p(x_{j}|\theta)\pi(\theta)\mathrm{d}\theta}$$
 2. $x$ 连续，$\theta$ 离散：$$\pi(\theta_{i}|x) =\frac{p(x|\theta_{i})\pi(\theta_{i})}{\sum_{i}p(x|\theta_{i})\pi(\theta_{i})} $$
 3. $x$ 离散，$\theta$ 离散：$$\pi(\theta_{i}|x_{j}) =\frac{p(x_{j}|\theta_{i})\pi(\theta_{i})}{\sum_{i}p(x_{j}|\theta_{i})\pi(\theta_{i})} $$

先验分布的确定十分关键，其原则有二：一是要根据先验信息；二是要使用方便，即在数学上处理方便。先验分布的确定有一些比较成熟的方法，如共轭先验分布法，此处不做详细讨论。

回到我们的核心目标，寻求参数 $\theta$ 的估计 $\hat{\theta}$ 只需要从后验分布 $\pi(\theta| x)$ 中合理提取信息即可。常用的提取方式是用**后验均方误差准则**，即选择这样的统计量
$$ \hat{\theta} = \hat{\theta}(x_{1},x_{2},\dots,x_{n}) $$ 使得后验均方误差达到最小，即
$$ \min\mathrm{MSE}(\hat{\theta} | x) =\min E^{\theta|x}(\hat{\theta}-\theta)^{2}$$ 这样的估计 $\hat{\theta}$ 称为 $\theta$ 的贝叶斯估计，其中 $E^{\theta|x}$ 表示用后验分布 $\pi(\theta|x)$ 求期望。求解上式并不困难，
$$\begin{split}
E^{\theta|x}(\hat{\theta}-\theta)^{2} &= \int_{\Theta}(\hat{\theta}-\theta)^{2}\pi(\theta | x)\mathrm{d}\theta \\
	&= \hat{\theta}^{2} -2\hat{\theta}\int_{\Theta}\theta\pi(\theta|x)\mathrm{d}\theta+\int_{\Theta}\theta^{2}\pi(\theta|x)\mathrm{d}\theta
\end{split}$$ 这是关于 $\hat{\theta}$ 的二次三项式，二次项系数为正，必有最小值：
$$\hat{\theta} = \int_{\Theta}\theta\pi(\theta|x)\mathrm{d}\theta=E(\theta|x)$$ 也就是说，在均方误差准则下， $\theta$ 的贝叶斯估计 $\hat{\theta}$ 就是 $\theta$ 的后验期望 $E(\theta|x)$。

类似的可证，在已知后验分布为 $\pi(\theta|x)$ 的情况下，参数函数 $g(\theta)$ 在均方误差下的贝叶斯估计为 $\hat{g}(\theta)=E[g(\theta)|x] $。

贝叶斯公式中，$m(x)$ 为样本的边际分布，它不依赖于 $\theta$ ，在后验分布计算中仅起到一个正则化因子的作用，加入把 $m(x)$ 省略，贝叶斯公式可改写为如下形式：
$$\pi(\theta|x) \propto p(x|\theta)\pi(\theta)$$ 上式右边虽然不是 $\theta$ 的密度函数或分布列，但在需要时利用正则化立即可以恢复密度函数或分布列原型。这时，可把上式右端称为**后验分布的核**，加入其中还有不含 $\theta$ 的因子，仍可剔去，使核更为精炼。
