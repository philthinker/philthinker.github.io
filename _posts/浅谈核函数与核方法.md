当输入空间为欧氏空间或离散集合，特征空间是希尔伯特（Hilbert）空间时，核函数（Kernel function）表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。
核函数的应用十分广泛，例如使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机，这样的方法称为核技巧（Kernel trick）。核技巧可应用于多种统计学问题。核方法（Kernel method）是一种比支持向量机更为一般的机器学习方法。

[TOC]

## 支持向量机简述

这里仅介绍支持向量机的基本原理与实现，不对细节做阐述。支持向量机（Support Vector Machine, SVM）是一种二分类模型。支持向量机的学习策略是间隔最大化，可以形式化为一个求解[凸二次规划](https://blog.csdn.net/philthinker/article/details/78510361)的问题，也等价于正则化的合页损失函数最小化的问题。支持向量机的学习方法是求解凸二次规划的最优化方法。

### 线性可分支持向量机

支持向量机学习的目标是在特征空间找到一个分离超平面，能将实例分到不同的类。分离超平面的方程为： $$w\cdot x + b =0$$ 它由法向量 $w$ 和截距 $b$ 决定。分离超平面将特征空间划分为两部分：一部分是正类，一部分是负类。法向量指向的一侧是正类。

一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。可以用
$$\gamma = y(w\cdot x + b)$$ 来表示分类的正确性及确信度，这就是**函数间隔**（functional margin）。一个集合到某个超平面的间隔用集合中最小的间隔表示。

选择分离超平面时，只有函数间隔还不够。因为只要成比例改变 $w$ 和 $b$ ，超平面并没有改变，但是间隔却会改变。因此我们可以对法向量 $w$ 加某些约束，如规范化，$\|w\|=1 $ 使得间隔是确定的。这时函数间隔成为**几何间隔**（geometric margin）。*支持向量机的学习思想是求解能够正确划分训练数据集且几何间隔最大的分离超平面*。具体地，这个问题可以表示为下面的约束最优化问题：
$$\begin{split} &\max_{w,b}\quad &\gamma \\ &\text{s.t. } & y_{i}\left( \frac{w}{\|w\|}\cdot x_{i} + \frac{b}{\|w\|} \right) \geq \gamma, i=1,2,\dots,N \end{split}$$ 取 $\gamma =1$进一步可以改写为下面线性可分支持向量机学习的最优化问题：
$$\begin{split} &\max_{w,b}\quad &\frac{1}{2}\|w\|^{2} \\ &\text{s.t. } & y_{i}\left(w\cdot x_{i} + b \right)-1 \geq 0, i=1,2,\dots,N \end{split}$$ 这是一个凸二次规划问题。求解此问题即可得到分类决策函数：$$f(x) = \mathrm{sign}(w^{*}\cdot x + b^{*})$$ 最大间隔超平面时唯一的。在线性可分的情况下，训练数据集中与分离超平面距离最近的样本点的实例称为**支持向量**（support vector）。即使约束式等号成立的点。在决定超平面时只有支持向量起作用。

应用[拉格朗日对偶性](https://blog.csdn.net/philthinker/article/details/66473983)，我们可以通过求解对偶问题得到原始问题的解。具体构造方法此处不详细讨论，这里仅给出学习方法：

 1. 构造并求解约束优化问题：$$\begin{split} \min_{\alpha} &\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}\alpha_{i} \\ \text{s.t. }& \sum_{i=1}^{N}\alpha_{i}y_{i} =0 \\ & \alpha_{i} \geq 0,\quad i =1,2,\dots,N \end{split}$$ 求得最优解 $\alpha^{*} = (\alpha_{1}^{*},\alpha_{2}^{*},\dots,\alpha_{N}^{*})^{T}$ 。
 2. 计算 $$w^{*} = \sum_{i=1}^{N}\alpha_{i}^{*} y_{i}x_{i}$$ 选择 $\alpha^{*}$ 的一个正分量 $\alpha_{j}^{*}>0$，计算 $$b^{*} = y_{j}-\sum_{i=1}^{N}\alpha_{i}^{*}y_{i}(x_{i}\cdot x_{j})$$
 3. 求出分离超平面 $$w^{*}\cdot x + b^{*} =0$$ 

### 软间隔最大化

当特征空间不是线性可分的空间时，上述方法中不等式约束并不都能成立。这时就需要修改间隔最大化，使其成为软间隔最大化。对每个样本点引入一个**松弛变量** $\xi_{i}\geq 0$，使得函数间隔加上松弛变量大于等于1。这样约束条件变成 $$y_{i}(w\cdot x_{i}+b) \geq 1-\xi_{i}$$ 目标函数变成 $$\frac{1}{2}\|w\|^{2}+C\sum_{i=1}^{N}\xi_{i}$$ 这里 $C>0$ 称为**惩罚参数**，一般由应用问题决定。$C$ 的值大时对误分类的惩罚大。上述最小化目标包含两层含义：

 1. 使间隔尽量大；
 2. 使误分类点的个数尽量小。

$C$ 是调和二者的系数。软间隔最优化问题的对偶问题是：选择惩罚参数 $C>0$，构造凸二次规划问题
$$\begin{split} \min_{\alpha} &\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j})-\sum_{i=1}^{N}\alpha_{i} \\ \text{s.t. }& \sum_{i=1}^{N}\alpha_{i}y_{i} =0 \\ & 0 \leq \alpha_{i} \leq C,\quad i =1,2,\dots,N \end{split}$$ 

## 核函数

线性不可分问题是假设有极少数几个样本在超平面分类错误一侧，我们引入松弛变量实际上放松了对分离超平面准确性的要求。然而现实中存在很多非线性可分问题（如某个圆内与圆外）用松弛变量无法得到需要的准确度。为解决此问题，我们可以用一个非线性变换将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。核技巧就属于这样的方法。

### 核函数的定义与判定

核函数的定义：

> 设 $\mathcal{X}$ 是输入空间（欧式空间或离散集合），又设 $\mathcal{H}$ 是特征空间（希尔伯特空间），如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射 $$\phi(x):\mathcal{X}\to \mathcal{H}$$ 使得对所有 $x,z\in\mathcal{X}$ ，函数 $K(x,z)$ 满足条件 $$K(x,z) = \phi(x)\cdot \phi(z)$$ 则称 $K(x,z)$ 为核函数，$\phi(x)$ 为映射函数，式中 $\phi(x)\cdot \phi(z)$ 为 $\phi(x)$ 和 $\phi(z)$ 的内积。

实际应用中，往往需要领域知识来直接选择核函数。核函数的选择需要验证有效性。那么问题来了，函数 $K(x,z)$ 满足什么条件才能成为核函数？通常所说的核函数即是**正定核函数**（positive definite kernel function）。正定核的充要条件如下：

> 设 $K:\mathcal{X}\times \mathcal{X} \to \mathbb{R}$ 是对称函数，则 $K(x,z)$ 为正定核函数的充要条件是对任意 $x_{i}\in\mathcal{X}, i=1,2,\dots,m$，$K(x,z)$ 对应的Gram矩阵：$$K=[K(x_{i},x_{j})]_{m\times m}$$ 是半正定矩阵。

### 常用核函数

实际问题中往往选用已知的核函数。

 1. 多项式核函数（polynomial kernel function）$$K(x,z) = (x\cdot z +1 )^{p}$$ 对应的支持向量机是一个 $p$ 次多项式分类器，在此情形下，分类决策函数是：$$f(x) = \mathrm{sign}\left( \sum_{i=1}^{N_{s}}a_{i}^{*}y_{i}(x_{i}\cdot x+1)^{p} + b^{*} \right)$$ 
 2. 高斯核函数（Gaussian kernel function）$$K(x,z) = \exp\left( -\frac{\|x-z\|^{2}}{2\sigma^{2}} \right)$$ 对应的支持向量机是高斯径向基函数分类器，在此情形下，分类决策函数是：$$f(x) = \mathrm{sign}\left( \sum_{i=1}^{N_{s}}a_{i}^{*}y_{i}\exp\left( -\frac{\|x-z\|^{2}}{2\sigma^{2}} \right) + b^{*} \right)$$

### 核技巧与非线性支持向量机

核技巧的想法是，在学习与预测中只定义核函数 $K(x,z)$ ，而不显示地定义映射函数。因为直接定义核函数比较容易。而且给定核函数后，特征空间与映射函数的取法不唯一。

核技巧应用到支持向量机，其基本想法是通过一个非线性变换将输入空间（欧式空间或离散集合）对应于一个特征空间（希尔伯特空间），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型。根据前面的内容我们可以看出：*在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积*。在对偶问题的目标函数中内积 $x_{i}\cdot x_{j}$ 可以用核函数 $K(x_{i},x_{j}) = \phi(x_{i})\cdot \phi(x_{j})$ 来代替。此时对偶问题的目标函数成为：
$$W(\alpha) = \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i},x_{j}) -\sum_{i=1}^{N}\alpha_{i}$$ 也就是说，**将线性支持向量机拓展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数**。同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式称为：
$$f(x) =\mathrm{sign}\left( \sum_{i=1}^{N_{s}}a_{i}^{*}y_{i}K(x_{i}, x) +b^{*} \right)$$   在核函数给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。

## 支持向量机的实现

支持向量机的学习问题可以形式化为求解凸二次规划问题。这样的凸二次规划具有全局最优解。但是当训练样本容量很大时，一般的优化算法往往非常低效，无法直接使用。目前常见的快速实现方法是**序列最小最优化**（sequential minimal optimization, SMO）。这里对其不做详细讨论。

除此之外，Joachims 实现的 SVM Light，以及Chang与Lin实现的LIBSVM软件包也被广泛使用。

- 感谢李航——《统计学习方法》清华大学出版社