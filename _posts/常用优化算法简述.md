本文介绍优化问题的基本概念与常用的优化算法，内容持续更新中。

@[toc] 

## 最优化问题简述

优化问题可概括为如下一般形式：

 - 求设计变量： $x_{1},x_{2},\dots,x_{n}$
 - 极小化目标函数： $f(x_{1},x_{2},\dots,x_{n})$
 - 满足约束条件：$$g_{u}(x_{1},x_{2},\dots,x_{n})\leq 0,\quad u=1,2,\dots,p$$ $$h_{v}(x_{1},x_{2},\dots,x_{n}) = 0,\quad v=1,2,\dots,m$$ 
根据是否满足约束条件可以把设计点分成**可行点**（又称内点）和**非可行点**（又称外点），根据约束边界是否通过某个设计点，又可将约束条件分成该设计点的**起作用约束**和**不起作用约束**。通常一个设计问题只有一个目标函数，这就是**单目标最优化问题**。

求解最优化问题的求解方法是针对比较复杂的极值问题提出的一种区别于解析法的数值法，或者说迭代算法。迭代法一般采用如下格式：$$X^{k+1} = X^{k} + \alpha S^{k}$$ 迭代算法产生的点列所对应的函数值严格地单调递减，并且最终收敛于最优化问题的极小点时，称此迭代算法具有**收敛性**。判断迭代点是否达到给定精度要求的判别式称为最优化算法的**终止准则**。常用的终止准则有以下三种：

 1. 点距准则：$$\|X^{k+1}-X^{k}\|\leq \varepsilon$$
 2. 值差准则：$$|f(X^{k})-f(X^{k+1})|\leq \varepsilon$$ 或者 $$\left| \frac{f(X^{k}) - f(X^{k+1})}{f(X^{k})} \right| \leq \varepsilon$$
 3. 梯度准则：$$\|\nabla f(X^{k+1})\|\leq\varepsilon$$

关于最优化研究的数学基础读者可参考[此文](https://blog.csdn.net/philthinker/article/details/78023085)，此处省略。

对于**无约束问题**，由微积分理论可知，一元函数 $f(x)$ 在 $x_{k}$ 处取得极值的必要条件是函数在该点的一阶导数等于零，充分条件是对应的二阶导数不等于零。类似的，多元函数 $f(X)$ 在点 $X^{k}$ 取得极值的必要条件是函数在该点的所有方向导数都等于零，即函数在该点的梯度等于零：$$\nabla f(X^{k}) = 0$$ 根据泰勒展开式，函数在该点的近似式整理后可得：$$f(X) - f(X^{k}) = \frac{1}{2}[X-X^{k}]^{T}\nabla^{2}f(X^{k})[X-X^{k}]$$ 当 $X^{k}$ 为函数的极小值点时，因为有 $f(X) - f(X^{k})>0$ ，故必有 $$[X-X^{k}]^{T}\nabla^{2}f(X^{k})[X-X^{k}] >0$$ 即函数的二阶导数矩阵必须是正定的，这就是多元函数取得极小值的充分条件。一般来说，上式对最优化问题只有理论意义，因为就实际问题而言，由于目标函数比较复杂，二阶导数矩阵不容易求得，正定性判断更加困难。因此，具体的最优化算法只将导数作为判断极小点的终止准则。关于无约束问题的极值条件等更详细内容请参考[此文](https://blog.csdn.net/philthinker/article/details/78191864)。

**约束问题**的极值条件比无约束问题复杂得多。下面就等式约束和不等式约束两种情况稍加讨论，详细讨论请参考[此文](https://blog.csdn.net/philthinker/article/details/78510361)。

对于**等式约束问题**：$$\min f(X)$$ $$\text{s.t. } h_{v}(X)=0,\quad v=1,2,\dots,m$$ 建立如下拉格朗日函数：$$L(X,\lambda) = f(X) + \sum_{v=1}^{m}\lambda_{v}h_{v}(X)$$ 令 $\nabla L(X,\lambda) = 0$ 得：$$\nabla f(X) + \sum_{v=1}^{m}\lambda_{v}\nabla h_{v}(X) = 0$$ $$\lambda_{v} \text{不全为零}$$ 这就是等式约束问题在点 $X$ 取得极值的必要条件，即**在等式约束的极值点上，目标函数的负梯度等于诸约束函数梯度的非零线性组合**。

对于**不等式约束问题**：$$\min f(X)$$ $$\text{s.t. } g_{u}(X)\leq 0,\quad u=1,2,\dots,p$$ 引入 $p$ 个松弛变量 $x_{n+u}\geq 0$ 可将不等式约束转化为等式约束：$$\min f(X)$$ $$\text{s.t. } g_{u}(X) + x_{n+u}^{2} = 0,\quad u=1,2,\dots,p$$ 建立这一问题的拉格朗日函数 $$L(X,\lambda, \bar{X}) = f(X) + \sum_{u=1}^{p}\lambda_{u}[g_{u}(X) + x_{n+u}^{2}]$$ 同样令其梯度为零，即 $\nabla L(X,\lambda, \bar{X}) = 0$ 则有 $$\begin{aligned} &\frac{\partial L}{\partial X} = \nabla f(X) + \sum_{u=1}^{p}\lambda_{u}\nabla g_{u}(X) = 0 \\ &\frac{\partial L}{\partial \lambda} = g_{u}(X) + x_{n+u}^{2} = 0 \\ &\frac{\partial L}{\partial \bar{X}} = 2\lambda_{u}x_{n+u} = 0,\quad u=1,2,\dots,p \end{aligned}$$ 不等式约束问题的极小点要么在可行域内，要么在约束边界上取得，其条件概括为 $$\nabla f(X) + \sum_{i\in I_{k}}\lambda_{i}\nabla g_{i}(X) = 0$$ $$\lambda_{i}\geq 0, i\in I_{k}$$ 式中 $g_{i}(X)\leq 0 (i\in I_{k})$ 为点 $X$ 的起作用约束。上述条件也成为**Kuhn-Tucker**条件，简称 K-T条件。该条件是约束问题极值的必要条件，其意义可概括为**在不等式约束问题的极小点上，目标函数的负梯度等于起作用约束梯度的非负线性组合**。在一般情况下，K-T点就是约束问题的最优点。

举个例子，用K-T条件判断点 $X^{k}=[2,0]^{T}$ 是否是一下问题的最优点：$$\min f(X) = (x_{1}-3)^{2} + x_{2}^{2}$$ $$\begin{aligned} \text{s.t.  }&g_{1}(X) = x_{1}^{2} + x_{2} -4\leq 0 \\&g_{2}(X) = -x_{2}\leq 0 \\ & g_{3}(X) = -x_{1}\leq 0\end{aligned}$$ 解：由于$$\begin{aligned} &g_{1}(X^{k}) =2^{2} + 0-4=0 \\ &g_{2}(X^{k})=0 \\ &g_{3}(X^{k})=-2 \end{aligned}$$ 可知点 $X^{k}$ 的起作用约束是 $g_{1}(X)\leq 0, g_{2}(X)\leq 0$，在点 $X^{k}$ 有 $$\begin{aligned} &\nabla f(X^{k}) = [-2,0]^{T} \\ &\nabla g_{1}(X^{k})= [4,1]^{T} \\ &\nabla g_{2}(X^{k}) = [0,-1]^{T} \end{aligned}$$ 将上式代入 K-T条件的第一式有 $$-\nabla f(X^{k}) = \lambda_{1}\nabla g_{1}(X^{k})+\lambda_{2}\nabla g_{2}(X^{k})$$ 解得 $\lambda_{1} = \lambda_{2} = 0.5$，均大于零，满足K-T条件，说明 $X^{k}$ 就是所给约束最优化问题的最优点。

## 无约束问题最优化方法

求解无约束优化问题的下降迭代解法具有统一的迭代格式，其基本的问题是选择搜索方向和在这些方向上进行一维搜索。根据搜索方式不同可将优化方法分为**导数法（或称解析法）** 和 **模式法（或称直接法）**两大类。

利用目标函数的一阶导数或二阶导数信息构造搜索方向的方法称为导数法。由于导数是函数变化的具体描述，因此导数法的收敛性和收敛速度都比较好。模式法通过几个已知点上的函数值的比较构造搜索方向，由于构成搜索方向的信息仅仅是几个有限点上的函数值，因此难以得到较为理想的搜索方向。

常见的导数法如梯度法、牛顿法、拟牛顿法和共轭梯度法可参考[此文](https://blog.csdn.net/philthinker/article/details/80615122)，此处从略。常见的模式法如鲍威尔法此处不作介绍。

## 线性规划

**目标函数和约束函数都是线性函数的最优化问题成为线性规划问题**，对应的算法成为线性规划算法。线性规划问题的可行域是一种封闭的凸多边形或凸多面体，它的最优解一般位于可行域的某一顶点上，而可行域的顶点是有限的。

线性规划问题的数学模型一般包含一组等式约束和一组设计变量的非负性约束：$$\min f(X) = \sum_{j=1}^{n}c_{j}x_{j}$$ $$\text{s.t. } \sum_{j=1}^{n}a_{ij}x_{j} = b_{i},\quad i=1,2,\dots,m$$ $$x_{j}\geq 0,\quad j=1,2,\dots,n$$ 或者写成向量形式：$$\min f(X) = C^{T}X$$ $$\text{s.t. } AX = B$$ $$x_{i}\geq 0,i=1,2,\dots,n$$ 一般情况下， $m<n$。如果实际问题中还存在其他不等式约束条件存在，则要在这些不等式约束条件中分别引入一个非负变量，使不等式变为等式，这种新加入的非负变量称为**松弛变量**。

求解线性规划问题最优解通常采用**单纯形算法**，这是一宗基于基本可行解的变换原理构成的一种线性规划算法，一般采用列表变换的形式进行，所列表格称为单纯形表，对应的算法亦称为单纯形表法。具体步骤此处不作介绍。

## 约束问题最优化方法

求解约束最优化问题的关键在于*如何处理各种约束条件*，其求解方法可分为直接法和间接法两类。在迭代过程中逐点考察约束，并使迭代点始终局限于可行域之内的算法成为**直接法**。把约束条件代入目标函数，使约束最优化问题转化为无约束最优化问题求解，或者将非线性问题转化为相对简单的二次规划问题或线性规划问题求解的算法称为**间接法**。

### 可行方向法（直接法）

可行方向法用来求解如下不等式约束问题：$$\min f(X)$$ $$\text{s.t. } g_{u}(X)\leq 0, u=1,2,\dots,p$$ 这种方法的基本思想是**从可行域内一个可行点出发，选择一个合适的搜索方向和步长因子，使下一个迭代点既不超出可行域，又使目标函数的值下降得尽可能多**。既使目标函数下降，又指向可行域内的下降可行方向 $S$ 必须同时满足以下关系：$$[\nabla f(X^{k})]^{T}S<0$$ $$[\nabla g_{i}(X^{k})]^{T}S<0$$ 式中 $I_{k}$ 为点 $X^{k}$ 的起作用约束的下标集合。

### 罚函数法（间接法）

罚函数法是一种将约束最优化问题转化为无约束最优化问题求解的算法。构造如下无约束问题：$$\min\phi(X,r_{k1},r_{k2}) = f(X) + r_{k1}G[g_{u}(X)] + r_{k2}H[h_{v}(X)]$$ 当点 $X$ 在可行域之外时，对目标函数值加以惩罚，或者当点 $X$ 位于约束边界附近时，这两项的函数值趋于无穷大，这相当于在约束边界筑起一道围墙，迫使迭代点在可行域内移动。

罚函数法分为**外点法**和**内点法**，以及结合二者的**混合法**。此处不作具体介绍，更多细节请参考[此文](https://blog.csdn.net/philthinker/article/details/78518153)。

### 乘子法（间接法）

惩罚函数法具有方法简单、使用方便等优点。但它存在固有的缺点，即*随着惩罚因子越来越趋向极限值，惩罚函数也变得越来越病态*，从而给计算带来了很多困难。乘子法可以克服此困难，其构造方法与罚函数法类似，具体介绍参考[此文](https://blog.csdn.net/philthinker/article/details/78518153)。

### 序列二次规划法（间接法）

序列二次规划（SQP）算法是*将复杂的非线性约束最优化问题转化为比较简单的二次规划问题求解的算法*。所谓的[二次规划](https://blog.csdn.net/philthinker/article/details/80615122)问题就是目标函数为二次函数，约束函数为线性函数的最优化问题。序列二次规划算法是目前求解非线性约束最优化问题的常用算法，此处不作详细介绍。

## 遗传算法

遗传算法是一种自适应全局最优化概率搜索算法。在遗传算法中，将设计变量 $X = [x_{1},x_{2},\dots,x_{n}]^{T}$ 用 $n$ 个同类编码，即：$$X:X_{1},X_{2},\dots,X_{n}$$ 其中每一个 $X_{i}$ 都是一个 $q$ 位编码符号串。符号串的每一位称为一个遗传基因，基因的所有可能取值称为**等位基因**，基因所在的位置称为该基因的**基因座**。于是，$X$ 就可以看作由 $n\times q$ 个遗传基因组成的染色体，也称个体 $X$。由 $m$ 个个体组成一个群体，记作 $P(t)(t=1,2,\dots,m)$ 。

最简单的等位基因由 $0$ 和 $1$ 表示，相应的染色体或个体就是一个二进制符号串，称为个体的**基因型**，与之对应的十进制数称为个体的**表现型**。遗传算法使用**适应度**这个概念来度量群体中各个个体的优劣程度，并以个体适应度的大小，通过选择运算决定哪些个体被淘汰，哪些个体遗传到下一代。再经过**交叉**和**变异**运算得到性能更加优秀的新个体和群体，最终得到最佳个体，即最优化问题的最优解。

### 遗传编码
把原问题的可行解转化为个体符号串的方法称为编码。最常用的方法是**二进制编码**。

二进制编码所构成的个体基因是一个二进制符号串，符号串长度与要求的精度有关。假设某一个参数的取值范围是 $[U_{min},U_{max}]$，若用长度为 $l$ 的二进制符号串来表示，总共产生 $2^{l}$ 个不同的编码。编码精度为：$$\delta = \frac{U_{max}-U_{min}}{2^{l}-1}$$ 假设某一个个体的编码是 $$X:b_{l}b_{l-1}\cdots b_{2}b_{1}$$ 则对应的解码公式是：$$x = U_{min}+\left(\sum_{i=1}^{l}b_{i}\cdot 2^{i-1}\right)\frac{U_{max}-U_{min}}{2^{l}-1}$$ 

### 个体适应度
适应度较高的个体遗传到下一代的概率较大。度量个体适应度的函数称为**适应度函数** $F(X)$ ，一般由目标函数或惩罚函数转换而来。例如对于极小化问题：$$\min f(X)$$ $$F(X)= \left\{\begin{aligned} &C_{max} - f(X),\quad &f(X)<C_{min} \\ &0, &f(X)\geq C_{max} \end{aligned}\right.$$

### 遗传运算
第 $t$ 代群体记作 $P(t)$，遗传算法的运算就是群体的反复演变的过程。对于群体 $P(t)$ ，对其进行选择、交叉和变异运算，以求得最优的个体，即问题的最优解。

**1. 选择运算**

遗传算法使用选择算子来对群体中的个体进行优胜劣汰操作。最常见的方法是**比例选择运算**，即*个体被选中并遗传到下一代的概率与它的适应度的大小成正比*。设群体大小为 $M$，个体 $i$ 的适应度为 $f_{i}$，则个体 $i$ 被选中的概率 $P_{is}$ 为：$$P_{is} = f_{i}/\sum_{i=1}^{M}f_{i}$$

**2. 交叉运算**

交叉运算模仿生物交配重组的过程，即两个个体交换部分基因，形成两个新生代个体。目前最常用的方法是**单点交叉运算**。![交叉](https://img-blog.csdn.net/20181013155333411?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BoaWx0aGlua2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70 =400x90)

**3. 变异运算**

变异运算将个体编码串中的某些基因座上的基因值用它的不同等位基因来替换，从而产生新的个体。最简单的方法是**基本位变异**。基本位变异操作是在个体编码串中依变异概率 $P_{s}$ 随机指定某一位或某几位基因座上的基因值作变异运算。
![变异](https://img-blog.csdn.net/20181013155946302?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BoaWx0aGlua2Vy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70 =350x60)
